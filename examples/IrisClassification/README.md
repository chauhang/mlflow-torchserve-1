# Deploying Iris Classification using TorchServe

The code, adapted from this [repository](http://chappers.github.io/2020/04/19/torch-lightning-using-iris/),
is almost entirely dedicated to distributed training (DDP), with the addition of a single mlflow.pytorch.autolog() call to enable automatic logging of params, metrics, and models.
Please note, as DDP broadcasts model states from rank 0 process to all other processes in the DDP constructor, one dosenâ€™t need to worry about multiple models being generated.
The `rank zero only decorator` of the autolog function enabled the saving the model generated by rank 0 process only.The model is trained using all the IRIS dataset features namely sepal-length,sepal-width,petal-length,petal-width. After deployment,
the model predicts the test input as belonging to one of the IRIS flower species namely SETOSA` , `VERSICOLOR`, `VIRGINICA`.

## Package Requirement

Install the required packages using the following command

`pip install -r requirements.txt`

### Running the code

To run the example via MLflow, navigate to the `examples/IrisClassification/` directory and run the command

```
mlflow run .

```

This will run `iris_classification.py` with the default set of parameters such as `--max_epochs=100`. You can see the default value in the MLproject file.

In order to run the file with custom parameters, run the command

```
mlflow run . -P max_epochs=X
```

where X is your desired value for max_epochs.

If you have the required modules for the file and would like to skip the creation of a conda environment, add the argument --no-conda.

```
mlflow run . --no-conda
```

To run it in gpu, use the following command

```
mlflow run . -P gpus=2 -P accelerator="ddp"
```
## Starting TorchServe

Create an empty directory `model_store` and run the following command to start torchserve.

`torchserve --start --model-store model_store`

## Creating a new deployment

Run the following command to create a new deployment named `iris_test`

The `index_to_name.json` file is the mapping file, which will convert the discrete output of the model to one of the flower species
based on the predefined mapping.

`mlflow deployments  create --name iris_test --target torchserve --model-uri iris.pt -C "MODEL_FILE=iris_classification.py" -C "HANDLER=iris_handler.py" -C "EXTRA_FILES=index_to_name.json"`

## Running prediction based on deployed model

IrisClassification model takes 4 different parameters - sepal length, sepal width, petal length and petal width.These parameters can be passed as tensor. For ex: `[4.4000, 3.0000, 1.3000, 0.2000]`.

For testing [iris dataset](http://archive.ics.uci.edu/ml/datasets/Iris/), we are going to use a sample input tensor placed in `sample.json` file.

Run the following command to invoke prediction of our sample input, whose output is stored in output.json file.

`mlflow deployments predict --name iris_test --target torchserve --input-path sample.json  --output-path output.json`

The model will classify the flower species based on the input test data as one among the three types and store it in `output.json`

